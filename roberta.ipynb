{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### hyperparameters #####\n",
    "ROBERTA_MODEL = 'roberta-base'\n",
    "FAST_TOKENIZER = True\n",
    "UPSAMPLE = False\n",
    "UPSAMPLE_FACTOR = 10 # only required when UPSAMPLE = True; 1 does not upsample, 2 doubles minority class, etc.\n",
    "PADDING = True\n",
    "TRUNCATION = True\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "TRAIN_EPOCH = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from dont_patronize_me import DontPatronizeMe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### cleanup #####\n",
    "print(\"Cleaning up...\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Cleanup done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(roberta_model, fast=True):\n",
    "\n",
    "    print(\"Preparing model...\")\n",
    "\n",
    "    if fast:\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(roberta_model)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(roberta_model)\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(roberta_model)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    print(\"Model prepared!\")\n",
    "\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset_from_csv(tokenizer):\n",
    "\n",
    "    # training data set comes from csv file, test set from official dev set\n",
    "    print(\"Loading datasets...\")\n",
    "\n",
    "    dpm = DontPatronizeMe('.', '.')\n",
    "    dpm.load_task1()\n",
    "\n",
    "    # get test set\n",
    "    teids = pd.read_csv('dev_semeval_parids-labels.csv')\n",
    "    teids.par_id = teids.par_id.astype(str)\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(teids)):\n",
    "        parid = teids.par_id[idx]\n",
    "        # select row from original dataset\n",
    "        text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
    "        label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'text':text,\n",
    "            'label':label\n",
    "        })\n",
    "\n",
    "    tedf1 = pd.DataFrame(rows)\n",
    "\n",
    "    # get train set\n",
    "    trdf1 = pd.read_csv('augmented_data.csv', sep='\\t', names=['label', 'text'])\n",
    "    # convert to int, error otherwise\n",
    "    trdf1['label'] = pd.to_numeric(trdf1['label'], errors='coerce')\n",
    "\n",
    "    # shuffle only training dataset\n",
    "    trdf1 = trdf1.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # convert to numpy\n",
    "    trdf1 = trdf1.to_numpy()\n",
    "    tedf1 = tedf1.to_numpy()\n",
    "\n",
    "    # posts and labels: data currently organised as (par_id | text | label) for test set\n",
    "    #                                               (label | text) for augmented training set\n",
    "    trposts = [row[1] for row in trdf1]\n",
    "    trlabels = [row[0] for row in trdf1]\n",
    "    teposts = [row[1] for row in tedf1]\n",
    "    telabels = [row[2] for row in tedf1]\n",
    "\n",
    "    # perform encoding\n",
    "    encodings_trn = tokenizer(trposts, padding=PADDING, truncation=TRUNCATION)\n",
    "    encodings_tst = tokenizer(teposts, padding=PADDING, truncation=TRUNCATION)\n",
    "\n",
    "    # convert to Dataset\n",
    "    dataset_trn = PCLDataset(encodings_trn, trlabels)\n",
    "    dataset_tst = PCLDataset(encodings_tst, telabels)\n",
    "\n",
    "    print(\"Datasets loaded!\")\n",
    "\n",
    "    return dataset_trn, dataset_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(tokenizer):\n",
    "\n",
    "    # both training and test data sets come from official data\n",
    "    print(\"Loading datasets...\")\n",
    "\n",
    "    dpm = DontPatronizeMe('.', '.')\n",
    "    dpm.load_task1()\n",
    "\n",
    "    trids = pd.read_csv('train_semeval_parids-labels.csv')\n",
    "    teids = pd.read_csv('dev_semeval_parids-labels.csv')\n",
    "    trids.par_id = trids.par_id.astype(str)\n",
    "    teids.par_id = teids.par_id.astype(str)\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(trids)):\n",
    "        parid = trids.par_id[idx]\n",
    "        # select row from original dataset to retrieve `text` and binary label\n",
    "        text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
    "        label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'text':text,\n",
    "            'label':label\n",
    "        })\n",
    "\n",
    "    trdf1 = pd.DataFrame(rows)\n",
    "\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(teids)):\n",
    "        parid = teids.par_id[idx]\n",
    "        # select row from original dataset\n",
    "        text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
    "        label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'text':text,\n",
    "            'label':label\n",
    "        })\n",
    "\n",
    "    tedf1 = pd.DataFrame(rows)\n",
    "\n",
    "    if UPSAMPLE and UPSAMPLE_FACTOR > 1:\n",
    "        upsampled_tr = trdf1\n",
    "        for _ in range(UPSAMPLE_FACTOR - 1):\n",
    "            upsampled_tr = upsampled_tr.append(trdf1.loc[trdf1['label'] == 1])\n",
    "        trdf1 = upsampled_tr\n",
    "\n",
    "    # shuffle only training dataset\n",
    "    trdf1 = trdf1.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # convert to numpy\n",
    "    trdf1 = trdf1.to_numpy()\n",
    "    tedf1 = tedf1.to_numpy()\n",
    "\n",
    "    # posts and labels: data currently organised as (par_id | text | label)\n",
    "    trposts = [row[1] for row in trdf1]\n",
    "    trlabels = [row[2] for row in trdf1]\n",
    "    teposts = [row[1] for row in tedf1]\n",
    "    telabels = [row[2] for row in tedf1]\n",
    "\n",
    "    # perform encoding\n",
    "    encodings_trn = tokenizer(trposts, padding=PADDING, truncation=TRUNCATION)\n",
    "    encodings_tst = tokenizer(teposts, padding=PADDING, truncation=TRUNCATION)\n",
    "\n",
    "    # convert to Dataset\n",
    "    dataset_trn = PCLDataset(encodings_trn, trlabels)\n",
    "    dataset_tst = PCLDataset(encodings_tst, telabels)\n",
    "\n",
    "    print(\"Datasets loaded!\")\n",
    "\n",
    "    return dataset_trn, dataset_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_device():\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = prepare_model(roberta_model=ROBERTA_MODEL, fast=FAST_TOKENIZER)\n",
    "dataset_trn, dataset_tst = prep_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = prep_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_trn = DataLoader(dataset_trn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "loader_tst = DataLoader(dataset_tst, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optim = AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress bar\n",
    "num_training_steps = TRAIN_EPOCH * len(loader_trn)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# start training\n",
    "for epoch in range(TRAIN_EPOCH):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in loader_trn:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    ##### evaluating model #####\n",
    "    print(\"Evaluating model...\")\n",
    "    print(\"Epoch\", epoch)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_labels_full = []\n",
    "        true_labels_full = []\n",
    "        for batch in loader_tst:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = F.softmax(outputs.logits, dim=-1)\n",
    "            predictions = torch.argmax(predictions, dim=-1)\n",
    "\n",
    "            pred_labels = [i.item() for i in predictions]\n",
    "            true_labels = [i.item() for i in labels]\n",
    "            pred_labels_full.extend(pred_labels)\n",
    "            true_labels_full.extend(true_labels)\n",
    "        # print metrics\n",
    "        print(\"Confusion Matrix\")\n",
    "        print(confusion_matrix(true_labels_full, pred_labels_full))\n",
    "        print(\"F1 Score\")\n",
    "        print(f1_score(true_labels_full, pred_labels_full))\n",
    "\n",
    "print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
